{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268a44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importowanie bibliotek\n",
    "import cv2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "from xy_dataset import XYDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6bf698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pobieranie wartości x z nazwy pliku\n",
    "def get_x(path, width):\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "#pobieranie wartości y z nazwy pliku\n",
    "def get_y(path, height):\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('ad_dataset_sala230', random_hflips=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8515643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33c2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c832fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet18\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654af5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf85ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.481239, 0.280518\n",
      "0.052267, 0.029087\n",
      "0.042583, 0.053563\n",
      "0.038394, 0.016149\n",
      "0.024998, 0.010291\n",
      "0.028118, 0.053651\n",
      "0.029428, 0.017180\n",
      "0.026256, 0.015141\n",
      "0.023554, 0.028529\n",
      "0.018301, 0.018918\n",
      "0.016252, 0.020275\n",
      "0.019747, 0.010506\n",
      "0.011272, 0.011312\n",
      "0.015047, 0.018193\n",
      "0.016508, 0.012344\n",
      "0.014238, 0.022125\n",
      "0.014575, 0.018530\n",
      "0.015859, 0.015652\n",
      "0.010976, 0.009908\n",
      "0.011024, 0.017384\n",
      "0.009074, 0.015173\n",
      "0.009046, 0.013238\n",
      "0.008858, 0.014485\n",
      "0.006909, 0.011378\n",
      "0.006991, 0.014336\n",
      "0.007324, 0.009767\n",
      "0.005660, 0.012550\n",
      "0.007806, 0.011604\n",
      "0.010171, 0.015134\n",
      "0.007556, 0.008562\n",
      "0.006391, 0.009306\n",
      "0.005975, 0.009602\n",
      "0.005898, 0.014061\n",
      "0.005439, 0.009432\n",
      "0.005179, 0.008774\n",
      "0.007113, 0.009600\n",
      "0.005027, 0.007475\n",
      "0.005001, 0.011622\n",
      "0.006635, 0.008443\n",
      "0.004586, 0.009061\n",
      "0.008431, 0.011294\n",
      "0.005325, 0.008909\n",
      "0.006443, 0.012192\n",
      "0.006359, 0.008450\n",
      "0.005162, 0.013921\n",
      "0.007076, 0.008460\n",
      "0.004640, 0.013832\n",
      "0.003775, 0.011598\n",
      "0.003367, 0.008132\n",
      "0.004805, 0.009527\n",
      "0.005561, 0.011734\n",
      "0.004535, 0.014050\n",
      "0.004102, 0.014050\n",
      "0.005046, 0.011288\n",
      "0.007134, 0.009791\n",
      "0.003974, 0.007790\n",
      "0.003518, 0.008334\n",
      "0.004374, 0.011708\n",
      "0.003944, 0.011201\n",
      "0.003426, 0.008778\n",
      "0.003720, 0.008566\n",
      "0.002697, 0.006438\n",
      "0.002655, 0.007995\n",
      "0.003078, 0.008937\n",
      "0.004258, 0.015556\n",
      "0.002130, 0.009431\n",
      "0.002643, 0.009606\n",
      "0.002394, 0.008900\n",
      "0.003431, 0.011020\n",
      "0.004088, 0.007587\n",
      "0.004091, 0.010322\n",
      "0.004892, 0.009348\n",
      "0.002904, 0.011396\n",
      "0.005377, 0.011535\n",
      "0.004118, 0.011412\n",
      "0.005286, 0.009229\n",
      "0.002860, 0.016397\n",
      "0.002972, 0.010764\n",
      "0.003489, 0.010640\n",
      "0.002672, 0.009577\n",
      "0.002680, 0.008953\n",
      "0.003043, 0.012370\n",
      "0.003593, 0.007826\n",
      "0.004061, 0.009850\n",
      "0.002347, 0.011693\n",
      "0.004172, 0.008774\n",
      "0.002669, 0.006870\n",
      "0.003069, 0.011827\n",
      "0.004439, 0.010761\n",
      "0.002966, 0.008501\n",
      "0.002532, 0.009211\n",
      "0.004505, 0.008078\n",
      "0.003717, 0.008881\n",
      "0.003316, 0.010840\n",
      "0.002853, 0.007885\n",
      "0.001766, 0.008055\n",
      "0.002317, 0.009263\n",
      "0.003884, 0.011912\n",
      "0.005001, 0.011169\n",
      "0.002881, 0.008689\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BEST_MODEL_PATH = 'steering_model_sala230_100.pth'\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print('%f, %f' % (train_loss, test_loss))\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd6c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
